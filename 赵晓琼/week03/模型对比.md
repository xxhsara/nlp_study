正则
- 优点：
    - 完全可控：规则由人工编写，结果可预测、可解释，出现问题能精准定位；
    - 速度极快：纯字符匹配，计算成本低，毫秒级处理大量文本；
    - 零数据依赖：无需标注数据，只要规则写的准就能用；
    - 落地成本低：入门门槛低，懂基础正则语法就能上手。
- 缺点：
    - 泛化能力极差：只能匹配规则覆盖的场景，稍有变体就失效；
    - 维护成本高：文本场景越复杂，规则越多越冗余，易出现“规则冲突”；
    - 无法处理语义： 只能匹配“形式”，无法理解“含义”（如无法区分“苹果手机”和“苹果水果”）；
    - 仅适用于结构化/半结构化文本：对无规则的自然语言（如日常对话）几乎无效；

TF-IDF
- 优点
    - 简单易理解：原理是基础统计，无需复杂算法背景就能掌握；
    - 计算高效：基于词频统计，训练和推理速度快，资源消耗低；
    - 无训练依赖：无需标注数据，仅需文本语料即可生成特征；
    - 可解释性强：能明确看到哪些词汇对文档分类/检索的贡献大。
- 缺点
    - 忽略语义：仅关注词的频率，无法处理一词多义、多词一义（如“电脑"和“计算机”视为不同词）；
    - 忽略上下文：是“词袋模型”，丢失词汇的顺序和语法信息（如“我打他”和“他打我”特征相同）。
    - 对低频词不友好：IDF会压低低频词权重，可能丢失关键信息；
    - 特征稀疏：词汇表量大时，特征向量维度极高，易出现维度灾难。

BERT
- 优点
    - 强语义理解：能捕捉上下文语义、一词多义、句法结构，远超传统方法；
    - 泛化能力强：预训练后微调即可适配分类、命名实体识别、问答等多任务；
    - 端到端学习：无需复杂的特征工程，直接从文本中学习有效特征；
    - 支持多语言：多语言版BERT可处理数十种语言的文本任务。
- 缺点
    - 计算成本高：模型参数量大(基础版1.1亿)，训练/推理需GPU，耗时久；
    - 数据依赖：微调需要大量标注数据，小样本场景效果差；
    - 可解释性差：“黑盒模型”，无法明确说明决策的具体原因；
    - 部署复杂：模型体积大，需优化（如量化、蒸馏）才能落地到边缘设备。

Prompt
- 优点
    - 小样本友好：仅需少量标注数据就能取得不错效果，解决BERT微调数据依赖问题；
    - 复用预训练能力：无需大幅微调模型，最大化利用预训练的语义知识；
    - 灵活适配任务：修改提示模版即可快速切换任务，无需重新训练；
    - 降低过拟合风险：小样本下比直接微调更稳定，不易过拟合。
- 缺点
    - 模版设计依赖经验：提示模版的质量直接决定效果，需人工反复调试；
    - 效果受表述影响：同一任务的不同模版表述（如“好/坏”vs“正面/负面”）可能导致效果波动；
    - 仍依赖大模型：本质是大模型的使用方法，仍有计算成本高、部署复杂的问题；
    - 复杂任务适配难：对问答、摘要等复杂任务，提示模版的设计难度大幅提升。

