# 单个模型分析

## BERT

``````python
# 核心组件
tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_PERTRAINED_PATH)
model = BertForSequenceClassification.from_pretrained(BERT_MODEL_PERTRAINED_PATH, num_labels=12)
model.load_state_dict(torch.load(BERT_MODEL_PKL_PATH))
model.to(device)
``````

优点：

1. 可以加载预训练权重
2. tokenizer自动处理上下文

缺点：

1. 模型参数多，占用显存
2. 每个batch都需要GPU计算，计算复杂度高

## Prompt+TF-IDF+GPT

优点：

1. 检索增强减少幻觉

``````python
# 提供相似样本作为上下文
dynamic_top10 = similar_row[1][0] + " -> " + similar_row[1][1] + "\n"
# 让GPT基于实际数据推理，减少编造答案的概率
``````

2. 支持动态类别

``````python
# 类别自动从数据中提取
"/".join(list(train_data[1].unique()))
# 新增类别只需更新数据集，无需修改模型结构
``````

3. 少样本/零样本能力强

``````python
# 仅需10个相似样本即可进行分类
top10_index = ids.toarray()[0].argsort()[::-1][:10]
# 不需要大规模训练数据，利用GPT的in-context learning能力
``````

缺点：

1. 每次调用的时候都计费
2. 稳定性依赖外部服务（API服务、网络......）
3. 串行处理，效率低下
4. 同义词问题：“电脑” ≠ “计算机”
5. 语义相似但词不同：“我想买手机” 、“我想购买手机”
6. 所有查询文本要发送到第三方服务器，可能造成隐私泄露

适用场景：

1. 需要快速验证分类任务可行性时，不需要训练的情况
2. 适用于类别动态变化的场景
3. 适用于标注数据极少的场景
4. 适用于复杂语义理解任务

## Regex Rule

优点：

1. 速度极快，时间复杂度O(n)
2. 规则清晰，可以精确知道为什么分类到某个类别
3. 资源消耗极低
4. 部署简单
5. 稳定性强
6. 无训练数据需求

缺点：

1. 泛化能力极差
2. 规则库需要人工持续维护，进行增删，维护成本较高
3. 难以处理复杂模式

适用场景：

1. 规则明确，模式固定的提取任务。比如说，提取用户的邮箱、电话、身份证号。

2. 对实时性要求极高的场景

3. 资源极度受限的环境

4. 项目初期，冷启动阶段

   > 冷启动：

## TF-IDF+机器学习

优点：

1. 使用稀疏矩阵运算，速度快，效率高
2. 不需要GPU，纯CPU计算，资源消耗低
3. 训练数据需求适中，不需要大量标注数据（相比BERT需要大量数据）
4. 部署简单
5. 支持增量学习。可以不断添加新数据，无需重新训练整个模型

缺点：

1. 语义理解能力有限
2. 无法处理同义词问题
3. 停用词依赖性强
4. 特征空间维度高，可能过拟合
5. 无法处理词序和语法

适用场景：

1. 中等规模标注数据
2. 对速度要求高的在线服务
3. 资源受限环境
4. 文本长度较短且规范